# SynchroRaMa
### Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding

Phyo Thet Yee<sup>1</sup>, Dimitrios Kollias<sup>2</sup>, Sudeepta Mishra<sup>1</sup>, Abhinav Dhall<sup>3</sup> <br><br>
<sup>1</sup>Indian Institute of Technology Ropar, <sup>2</sup>Queen Mary University of London, <sup>3</sup>Monash University <br>

**Accepted at WACV 2026 Conference**

<br>
<p align="center">
  <img src="/content/synchrorama_teaser.png" alt="Teaser Image" width="600"/>
</p>
<br>

<p>
  <a href="https://arxiv.org/abs/xxxx.xxxxx">
    <img src="https://img.shields.io/badge/arXiv-Paper-red?style=flat&logo=arxiv" alt="arXiv">
  </a>
  <a href="https://novicemm.github.io/synchrorama">
    <img src="https://img.shields.io/badge/Project-Page-blue?style=flat&logo=google-chrome" alt="Project Page">
  </a>
</p>

## Code is coming soon!!!

## Citation
```bibtex
@misc{xxx,
  title={SynchroRaMa: Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding},
  author={},
  year={2026},
  eprint={},
  archivePrefix={arXiv},
  primaryClass={cs.CV}
}

